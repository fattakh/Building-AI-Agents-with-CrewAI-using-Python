# -*- coding: utf-8 -*-
"""Building AI Agents with CrewAI using Python

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tej1v0azUoN3u3vxTNcUJxc4emr9O8UU
"""

!pip uninstall -y litellm
!pip install 'litellm[proxy]' --force-reinstall --upgrade
!pip install crewai llama-cpp-python transformers accelerate langchain langchain-community --upgrade

import os
from llama_cpp import Llama

# Create the directory if it doesn't exist
model_dir = "download/models"
os.makedirs(model_dir, exist_ok=True)

# Define the model path
model_path = os.path.join(model_dir, "mistral-7b-instruct.Q4_K_M.gguf")

# Check if the model already exists, if not, download it
if not os.path.exists(model_path):
    print(f"Downloading model to {model_path}...")
    # Using wget to download the model from HuggingFace. Make sure to use the correct URL.
    # This URL is an example and might need to be adjusted if the model location changes.
    !wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf -O {model_path}
    print("Download complete.")
else:
    print(f"Model already exists at {model_path}.")

llm = Llama(
    model_path=model_path,
    n_ctx=2048,
    n_threads=4,
    n_gpu_layers=0
)

def local_llm(prompt: str) -> str:
    output = llm(prompt, max_tokens=500, stop=["</s>"])
    return output["choices"][0]["text"].strip()

# This cell is no longer needed as LlamaCpp is initialized directly in the Agent setup
# You can remove or comment out its content if desired.

import os
from crewai import Agent
from langchain_core.language_models.llms import BaseLLM

# Define the model path here to ensure it's accessible
model_dir = "download/models"
model_path = os.path.join(model_dir, "mistral-7b-instruct.Q4_K_M.gguf")

# NOTE: llm (llama_cpp.Llama instance) is already initialized in cell BXpLU_nkJTuX
# This custom class directly uses that instance.
class CustomLlamaLLM(BaseLLM):
    llm: Llama

    @property
    def _llm_type(self) -> str:
        return "custom_llama"

    def _call(self, prompt: str, stop=None, **kwargs) -> str:
        output = self.llm(prompt, max_tokens=500, stop=stop or ["</s>"], **kwargs)
        return output["choices"][0]["text"].strip()

    @property
    def _identifying_params(self):
        return {"model_path": self.llm.model_path}

# Instantiate our custom LLM wrapper
custom_llm_wrapper = CustomLlamaLLM(llm=llm)

# Now, create CrewAI agents using this custom LLM wrapper
researcher = Agent(
    role='Market Research Analyst',
    goal='Analyze competitors and summarize their marketing strategies',
    backstory='An expert in market intelligence and competitive analysis.',
    llm=custom_llm_wrapper,
    allow_delegation=False
)

writer = Agent(
    role='Content Strategist',
    goal='Use research to create a compelling marketing strategy document',
    backstory='A seasoned content strategist with a flair for storytelling.',
    llm=custom_llm_wrapper
)

from crewai import Task

task1 = Task(
    description="List top 3 competitors and their marketing strategies based on current trends.",
    agent=researcher,
    expected_output="A summary of 3 competitors with key marketing strategies."
)

task2 = Task(
    description="Create a content marketing strategy based on the competitor summary.",
    agent=writer,
    expected_output="A structured document with our content strategy inspired by competitors.",
    depends_on=[task1]
)

from crewai import Crew

crew = Crew(
    agents=[researcher, writer],
    tasks=[task1, task2],
    verbose=True  # see what each agent does
)

result = crew.kickoff()
print(result)

pip install apscheduler

